{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "from pathlib import Path\n",
    "import collections\n",
    "import pandas as pd\n",
    "import pandas.io.sql as pandasql\n",
    "from psycopg2.extras import execute_values\n",
    "from psycopg2 import connect\n",
    "import numpy\n",
    "import matplotlib as mpl\n",
    "from IPython.display import HTML, display\n",
    "import rick\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from setuptools import setup, find_packages\n",
    "CONFIG = configparser.ConfigParser()\n",
    "CONFIG.read(str(Path.home().joinpath('db.cfg')))\n",
    "dbset = CONFIG['DBSETTINGS']\n",
    "con = connect(**dbset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_set = pd.read_sql('''select * from congestion.route_nextbest_ordered \n",
    "limit 10              \n",
    "                          ''',con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   start_vid   end_vid  rank  segment_id  \\\n",
      "0   30414921  30414866     1        2067   \n",
      "1   30414921  30414866     2        2067   \n",
      "\n",
      "                                           link_set     length  \\\n",
      "0  [8374555211, 8264120121, 8264397781, 8264120091]  28.079778   \n",
      "1  [8374555211, 8264120121, 8264397781, 8264120091]  28.079778   \n",
      "\n",
      "                                                geom  pot_seg  pot_start_vid  \\\n",
      "0  0102000020E61000000500000082E2C798BBD853C048F9...     2072       30414923   \n",
      "1  0102000020E61000000500000082E2C798BBD853C048F9...     2038       30414866   \n",
      "\n",
      "   pot_end_vid                                       pot_link_set  pot_length  \\\n",
      "0     30414921                           [8287056821, 8287056811]  130.298716   \n",
      "1     30414610  [295889901, 295889321, 295890151, 295889691, 2...  450.005186   \n",
      "\n",
      "                                            pot_geom  ang_diff     sum  \n",
      "0  0102000020E610000003000000BF0E9C33A2D853C0DE93...  0.873724  7308.0  \n",
      "1  0102000020E610000009000000CDAFE600C1D853C02C48...  1.161242  7308.0  \n"
     ]
    }
   ],
   "source": [
    "unique_set = merge_set[['start_vid', 'end_vid']].drop_duplicates().iloc[0]\n",
    "prepare_set = merge_set[(merge_set['start_vid'] == start_vid) & (merge_set['end_vid'] == end_vid)]\n",
    "print(prepare_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_usage(link_set, link_used):\n",
    "    result = []\n",
    "    if link_used == []:\n",
    "        return False\n",
    "    for i in link_used:\n",
    "        if collections.Counter(link_set) == collections.Counter(i):\n",
    "            result.append(True)\n",
    "        else:\n",
    "            result.append(False)\n",
    "    if True in result:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_selection(segment_set, min_angle, angle_diff, length_tolerance):\n",
    "    ## find out number of ranks\n",
    "    if pd.isna(segment_set['ang_diff'].iloc[0]):\n",
    "        rank = 0\n",
    "    else:\n",
    "        if segment_set['ang_diff'].iloc[0] < min_angle:\n",
    "            if segment_set.shape[0] == 2:\n",
    "                rank1=segment_set[(segment_set['rank']==1)]\n",
    "                rank2=segment_set[(segment_set['rank']==2)]\n",
    "\n",
    "                if rank2['ang_diff'].iloc[0] - rank1['ang_diff'].iloc[0] < angle_diff and rank2['pot_length'].iloc[0] - rank1['pot_length'].iloc[0] < length_tolerance:\n",
    "                    rank = 2\n",
    "                else:\n",
    "                    rank = 1\n",
    "            elif segment_set.shape[0] == 1:\n",
    "                    rank = 1\n",
    "        else:\n",
    "            rank =0\n",
    "       \n",
    "    return rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_chosen_segments(segment_set, rank):\n",
    "    if rank != 0:\n",
    "        whole_set = segment_set[(segment_set['rank']==rank)].iloc[0]\n",
    "        original_set = whole_set[['start_vid', 'end_vid', 'segment_id', 'link_set', 'length']]\n",
    "        additional_set = whole_set[['pot_start_vid', 'pot_end_vid', 'pot_seg', 'pot_link_set', 'pot_length']]\n",
    "        # find which node to use for new start and end_vid\n",
    "        if original_set['start_vid'] == additional_set['pot_end_vid']:\n",
    "            new_start_vid = additional_set['pot_start_vid']\n",
    "            new_end_vid = original_set['end_vid']\n",
    "        else:\n",
    "            new_start_vid = original_set['start_vid']\n",
    "            new_end_vid = additional_set['pot_end_vid'] \n",
    "        new_length = additional_set['pot_length'] + original_set['length']\n",
    "        new_link_set = additional_set['pot_link_set'] + original_set['link_set']\n",
    "        additional_links = additional_set['pot_link_set']\n",
    "    else:\n",
    "        new_start_vid = original_set['start_vid']\n",
    "        new_end_vid = original_set['end_vid']\n",
    "        new_length = original_set['length']\n",
    "        new_link_set = original_set['link_set']\n",
    "        additional_links = []\n",
    "    return new_start_vid, new_end_vid, new_length, new_link_set, additional_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30414923\n"
     ]
    }
   ],
   "source": [
    "rank = make_selection(prepare_set,25, 5, 50)\n",
    "new_start_vid, new_end_vid, new_length, new_link_set, additional_links = return_chosen_segments(prepare_set, rank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30414921 30414866\n",
      "[8374555211, 8264120121, 8264397781, 8264120091]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['pot_seg, pot_start_vid, pot_end_vid, pot_length, ang_diff'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-475-7cdda2f08c58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mlink_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'link_set'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlink_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mprepare_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pot_seg, pot_start_vid, pot_end_vid, pot_length, ang_diff'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/etc/jupyterhub/.venv/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2932\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2933\u001b[0m             indexer = self.loc._convert_to_indexer(key, axis=1,\n\u001b[0;32m-> 2934\u001b[0;31m                                                    raise_missing=True)\n\u001b[0m\u001b[1;32m   2935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/etc/jupyterhub/.venv/lib/python3.5/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter, raise_missing)\u001b[0m\n\u001b[1;32m   1352\u001b[0m                 kwargs = {'raise_missing': True if is_setter else\n\u001b[1;32m   1353\u001b[0m                           raise_missing}\n\u001b[0;32m-> 1354\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1355\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/etc/jupyterhub/.venv/lib/python3.5/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1159\u001b[0m         self._validate_read_indexer(keyarr, indexer,\n\u001b[1;32m   1160\u001b[0m                                     \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m                                     raise_missing=raise_missing)\n\u001b[0m\u001b[1;32m   1162\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/etc/jupyterhub/.venv/lib/python3.5/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1244\u001b[0m                 raise KeyError(\n\u001b[1;32m   1245\u001b[0m                     u\"None of [{key}] are in the [{axis}]\".format(\n\u001b[0;32m-> 1246\u001b[0;31m                         key=key, axis=self.obj._get_axis_name(axis)))\n\u001b[0m\u001b[1;32m   1247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m             \u001b[0;31m# We (temporarily) allow for some missing keys with .loc, except in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['pot_seg, pot_start_vid, pot_end_vid, pot_length, ang_diff'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "link_used = []\n",
    "new_sets = []\n",
    "rows = []\n",
    "unique_set = merge_set[['start_vid', 'end_vid']].drop_duplicates()\n",
    "merge_num = 0\n",
    "for index, key in unique_set.reset_index(drop=True).iterrows(): \n",
    "    \n",
    "    start_vid = unique_set['start_vid'].iloc[index]\n",
    "    end_vid = unique_set['end_vid'].iloc[index]\n",
    "    prepare_set = merge_set[(merge_set['start_vid'] == start_vid) & (merge_set['end_vid'] == end_vid)]\n",
    "\n",
    "    if prepare_set['length'] < 100:\n",
    "        rank = make_selection(prepare_set, 25,5,50)\n",
    "        new_start_vid, new_end_vid, new_length, new_link_set, additional_links = return_chosen_segments(prepare_set, rank)\n",
    "        link_used.append(additional_links)\n",
    "        link_used.append(new_link_set)\n",
    "        \n",
    "        new_sets.append(new_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'new_start_vid'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/etc/jupyterhub/.venv/lib/python3.5/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2656\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2657\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'new_start_vid'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-470-2bb679a8ad6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcheck_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlink_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlink_used\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# if this intersection has an next best one:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'new_start_vid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m# if there are more than one rank and length is less than 100 or something\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/etc/jupyterhub/.venv/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2926\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2927\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/etc/jupyterhub/.venv/lib/python3.5/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2657\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2659\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'new_start_vid'"
     ]
    }
   ],
   "source": [
    "link_used = []\n",
    "new_sets = []\n",
    "rows = []\n",
    "unique_set = merge_set[['start_vid', 'end_vid']].drop_duplicates()\n",
    "merge_num = 0\n",
    "# loop through unique sets of intersection\n",
    "# !!this will be ordered by priority and length eventually!!\n",
    "for index, key in unique_set.reset_index(drop=True).iterrows(): \n",
    "    \n",
    "    start_vid = unique_set['start_vid'].iloc[index]\n",
    "    end_vid = unique_set['end_vid'].iloc[index]\n",
    "    #print(start_vid, end_vid)\n",
    "    prepare_set = merge_set[(merge_set['start_vid'] == start_vid) & (merge_set['end_vid'] == end_vid)]\n",
    "    link_set = prepare_set['link_set'].iloc[0]\n",
    "    \n",
    "    # Check if this link_set was used to match another set before\n",
    "    # if not, selection best and merge\n",
    "    if check_usage(link_set, link_used) == False:\n",
    "        # if this intersection has an next best one: \n",
    "        if pd.isna(prepare_set['new_start_vid'].iloc[0]) == False:\n",
    "            \n",
    "            # if there are more than one rank and length is less than 100 or something         \n",
    "            if prepare_set['length'].iloc[0] < 100:    \n",
    "                # if there are two possibility for this intersection set\n",
    "                # then compare which one is better\n",
    "                if prepare_set['rank'].shape[0] == 2: \n",
    "                    rank1=prepare_set[(merge_set['rank']==1)]\n",
    "                    rank2=prepare_set[(merge_set['rank']==2)]\n",
    "                    print(prepare_set['rank'].shape[0])\n",
    "                    # find hausdorffdistance difference and length difference\n",
    "                    hausorffdiff = abs(rank1['st_hausdorffdistance'].iloc[0]/rank2['st_hausdorffdistance'].iloc[0])\n",
    "                    lengthdiff = rank1['new_length'].iloc[0] - rank2['new_length'].iloc[0]\n",
    "\n",
    "                    # make selection based on hausdorffdistance difference and length difference\n",
    "                    # could try and play with the params a little bit\n",
    "                    if hausorffdiff < 5 and lengthdiff > 20:\n",
    "                        select_set = 2\n",
    "                    else:\n",
    "                        select_set = 1\n",
    "\n",
    "                    selection = merge_set[(merge_set['rank']==select_set)&(merge_set['start_vid']==start_vid)&(merge_set['end_vid']==end_vid)]\n",
    "\n",
    "                    # find links that were used to merge, append it to another list\n",
    "                    # also have to find out if it was using a set of link that was used before\n",
    "                    old_links = selection['link_set'].iloc[0]\n",
    "                    new_links = selection['new_link_set'].iloc[0]\n",
    "                    additional_links = list(set(new_links).difference(old_links))\n",
    "\n",
    "                    if check_usage(additional_links,link_used) == False:\n",
    "                        new_set = np.array([selection['new_start_vid'].iloc[0], selection['new_end_vid'].iloc[0], selection['new_length'].iloc[0], selection['new_link_set'].iloc[0]])\n",
    "                        link_used.append(additional_links)\n",
    "                        link_used.append(old_links)\n",
    "                        new_sets.append(new_set)\n",
    "                        merge_num = merge_num + 1\n",
    "                    else:\n",
    "                        new_set = np.array([prepare_set['start_vid'].iloc[0], prepare_set['end_vid'].iloc[0], prepare_set['length'].iloc[0], prepare_set['link_set'].iloc[0]])\n",
    "                        link_used.append(old_links)\n",
    "                        new_sets.append(new_set)\n",
    "\n",
    "                # if there is only one possibility\n",
    "                # see if that possibility is viable (this has not been added)\n",
    "\n",
    "                elif prepare_set['rank'].shape[0] == 1:\n",
    "                    # if st_hausdorffdistance < maybe like 50 or something????: then use it, else append\n",
    "                    selection = merge_set[(merge_set['rank']==1)&(merge_set['start_vid']==start_vid)&(merge_set['end_vid']==end_vid)]\n",
    "                    old_links = selection['link_set'].iloc[0]\n",
    "                    new_links = selection['new_link_set'].iloc[0]\n",
    "                    additional_links = list(set(new_links).difference(old_links))\n",
    "\n",
    "                    if check_usage(additional_links, link_used) == False and selection['st_hausdorffdistance'] < 100:\n",
    "                        new_set = np.array([selection['new_start_vid'].iloc[0], selection['new_end_vid'].iloc[0], selection['new_length'].iloc[0], selection['new_link_set'].iloc[0]])\n",
    "                        link_used.append(old_links)\n",
    "                        link_used.append(additional_links)\n",
    "                        new_sets.append(new_set)\n",
    "                        merge_num = merge_num + 1\n",
    "                    else:\n",
    "                        new_set = np.array([prepare_set['start_vid'].iloc[0], prepare_set['end_vid'].iloc[0], prepare_set['length'].iloc[0], prepare_set['link_set'].iloc[0]])\n",
    "                        print('was used before')\n",
    "                        link_used.append(old_links)\n",
    "                        new_sets.append(new_set)\n",
    "\n",
    "            # if the set length is longer than 100 then \n",
    "            # check to see if it was used to merge \n",
    "            # if yes dont append, if not append \n",
    "            else: \n",
    "                print('longer than 100m')\n",
    "                old_set = np.array([prepare_set['start_vid'].iloc[0], prepare_set['end_vid'].iloc[0], prepare_set['length'].iloc[0], prepare_set['link_set'].iloc[0]])\n",
    "                new_sets.append(old_set)\n",
    "                link_used.append(link_set)\n",
    "                \n",
    "        # if it was not used and does not have next best, append it \n",
    "        else:\n",
    "            print('does not have next best')\n",
    "            old_set = np.array([prepare_set['start_vid'].iloc[0], prepare_set['end_vid'].iloc[0], prepare_set['length'].iloc[0], prepare_set['link_set'].iloc[0]])\n",
    "            new_sets.append(old_set)\n",
    "            link_used.append(link_set)\n",
    "    # if link_set as used, continue and dont append\n",
    "    else:\n",
    "        print('used')\n",
    "\n",
    "for i in range(len(new_sets)):\n",
    "    item = new_sets[i]\n",
    "    length = item[2].astype(float)\n",
    "    row = (item[0].astype(float), item[1].astype(float), length, item[3])\n",
    "    rows.append(row)\n",
    "        \n",
    "#sql = '''insert into congestion.test_shortseg(start_vid, end_vid, length, link_set) VALUES %s'''    \n",
    "#with con:\n",
    "#    with con.cursor() as cur:\n",
    "#        execute_values(cur, sql, rows)    \n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
