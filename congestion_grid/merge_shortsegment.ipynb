{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "from pathlib import Path\n",
    "import collections\n",
    "import pandas as pd\n",
    "import pandas.io.sql as pandasql\n",
    "from psycopg2.extras import execute_values\n",
    "from psycopg2 import connect\n",
    "import numpy\n",
    "import matplotlib as mpl\n",
    "from IPython.display import HTML, display\n",
    "import rick\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from setuptools import setup, find_packages\n",
    "CONFIG = configparser.ConfigParser()\n",
    "CONFIG.read(str(Path.home().joinpath('db.cfg')))\n",
    "dbset = CONFIG['DBSETTINGS']\n",
    "con = connect(**dbset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_set = pd.read_sql('''select rank, start_vid, end_vid, length, link_set, new_start_vid, new_end_vid, new_length, new_link_set, st_hausdorffdistance  from congestion.route_nextbest_ordered \n",
    "                          limit 10\n",
    "                          ''',con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_usage(link_set, link_used):\n",
    "    result = []\n",
    "    #print('link_set' +str(link_set))\n",
    "    #print('link_used' +str(link_used))\n",
    "    if link_used == []:\n",
    "        return False\n",
    "    for i in link_used:\n",
    "        if collections.Counter(link_set) == collections.Counter(i):\n",
    "            result.append(True)\n",
    "        else:\n",
    "            result.append(False)\n",
    "    if True in result:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109555486 30420391\n",
      "2\n",
      "30420391 109555486\n",
      "2\n",
      "30460238 30460237\n",
      "2\n",
      "30460237 30460238\n",
      "2\n",
      "30431761 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/etc/jupyterhub/.venv/lib/python3.5/site-packages/ipykernel_launcher.py:26: UserWarning:\n",
      "\n",
      "Boolean Series key will be reindexed to match DataFrame index.\n",
      "\n",
      "/etc/jupyterhub/.venv/lib/python3.5/site-packages/ipykernel_launcher.py:27: UserWarning:\n",
      "\n",
      "Boolean Series key will be reindexed to match DataFrame index.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30431762\n",
      "2\n",
      "30431762 30431761\n",
      "2\n",
      "30415731 30415730\n",
      "2\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-415-63af12e88a2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rank'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0;31m# find hausdorffdistance difference and length difference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                 \u001b[0mhausorffdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrank1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'st_hausdorffdistance'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mrank2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'st_hausdorffdistance'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m                 \u001b[0mlengthdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrank1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'new_length'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mrank2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'new_length'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/etc/jupyterhub/.venv/lib/python3.5/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1500\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1502\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/etc/jupyterhub/.venv/lib/python3.5/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   2228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2229\u001b[0m             \u001b[0;31m# validate the location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2230\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2232\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/etc/jupyterhub/.venv/lib/python3.5/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   2137\u001b[0m         \u001b[0mlen_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen_axis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlen_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2139\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"single positional indexer is out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "link_used = []\n",
    "new_sets = []\n",
    "rows = []\n",
    "unique_set = merge_set[['start_vid', 'end_vid']].drop_duplicates()\n",
    "\n",
    "# loop through unique sets of intersection\n",
    "# !!this will be ordered by priority and length eventually!!\n",
    "for index, key in unique_set.reset_index(drop=True).iterrows(): \n",
    "    \n",
    "    start_vid = unique_set['start_vid'].iloc[index]\n",
    "    end_vid = unique_set['end_vid'].iloc[index]\n",
    "    #print(start_vid, end_vid)\n",
    "    prepare_set = merge_set[(merge_set['start_vid'] == start_vid) & (merge_set['end_vid'] == end_vid)]\n",
    "    link_set = prepare_set['link_set'].iloc[0]\n",
    "    \n",
    "    # Check if this link_set was used to match another set before\n",
    "    # if not, selection best and merge\n",
    "    if check_usage(link_set, link_used) == False:\n",
    "        # if this intersection has an next best one: \n",
    "        if prepare_set['new_start_vid'] is not None:\n",
    "            \n",
    "            # if there are more than one rank and length is less than 100 or something         \n",
    "            if prepare_set['length'].iloc[0] < 100:    \n",
    "                # if there are two possibility for this intersection set\n",
    "                # then compare which one is better\n",
    "                if prepare_set['rank'].shape[0] == 2: \n",
    "                    rank1=prepare_set[(merge_set['rank']==1)]\n",
    "                    rank2=prepare_set[(merge_set['rank']==2)]\n",
    "                    print(prepare_set['rank'].shape[0])\n",
    "                    # find hausdorffdistance difference and length difference\n",
    "                    hausorffdiff = abs(rank1['st_hausdorffdistance'].iloc[0]/rank2['st_hausdorffdistance'].iloc[0])\n",
    "                    lengthdiff = rank1['new_length'].iloc[0] - rank2['new_length'].iloc[0]\n",
    "\n",
    "                    # make selection based on hausdorffdistance difference and length difference\n",
    "                    # could try and play with the params a little bit\n",
    "                    if hausorffdiff < 5 and lengthdiff > 20:\n",
    "                        select_set = 2\n",
    "                    else:\n",
    "                        select_set = 1\n",
    "\n",
    "                    selection = merge_set[(merge_set['rank']==select_set)&(merge_set['start_vid']==start_vid)&(merge_set['end_vid']==end_vid)]\n",
    "\n",
    "                    # find links that were used to merge, append it to another list\n",
    "                    # also have to find out if it was using a set of link that was used before\n",
    "                    old_links = selection['link_set'].iloc[0]\n",
    "                    new_links = selection['new_link_set'].iloc[0]\n",
    "                    additional_links = list(set(new_links).difference(old_links))\n",
    "\n",
    "                    if check_usage(additional_links,link_used) == False:\n",
    "                        new_set = np.array([selection['new_start_vid'].iloc[0], selection['new_end_vid'].iloc[0], selection['new_length'].iloc[0], selection['new_link_set'].iloc[0]])\n",
    "                        link_used.append(additional_links)\n",
    "                        link_used.append(old_links)\n",
    "                        new_sets.append(new_set)\n",
    "                    else:\n",
    "                        new_set = np.array([prepare_set['start_vid'].iloc[0], prepare_set['end_vid'].iloc[0], prepare_set['length'].iloc[0], prepare_set['link_set'].iloc[0]])\n",
    "                        link_used.append(old_links)\n",
    "                        new_sets.append(new_set)\n",
    "\n",
    "                # if there is only one possibility\n",
    "                # see if that possibility is viable (this has not been added)\n",
    "\n",
    "                elif prepare_set['rank'].shape[0] == 1:\n",
    "                    # if st_hausdorffdistance < maybe like 50 or something????: then use it, else append\n",
    "                    selection = merge_set[(merge_set['rank']==1)&(merge_set['start_vid']==start_vid)&(merge_set['end_vid']==end_vid)]\n",
    "                    old_links = selection['link_set'].iloc[0]\n",
    "                    new_links = selection['new_link_set'].iloc[0]\n",
    "                    additional_links = list(set(new_links).difference(old_links))\n",
    "\n",
    "                    if check_usage(additional_links, link_used) == False:\n",
    "                        new_set = np.array([selection['new_start_vid'].iloc[0], selection['new_end_vid'].iloc[0], selection['new_length'].iloc[0], selection['new_link_set'].iloc[0]])\n",
    "                        link_used.append(old_links)\n",
    "                        link_used.append(additional_links)\n",
    "                        new_sets.append(new_set)\n",
    "                    else:\n",
    "                        # if it was used, maybe this can be added to that merged segment too? \n",
    "                        # if its too long it can just get partition up later\n",
    "                        new_set = np.array([prepare_set['start_vid'].iloc[0], prepare_set['end_vid'].iloc[0], prepare_set['length'].iloc[0], prepare_set['link_set'].iloc[0]])\n",
    "                        print('was used before')\n",
    "                        link_used.append(old_links)\n",
    "                        new_sets.append(new_set)\n",
    "\n",
    "            # if the set length is longer than 100 then \n",
    "            # check to see if it was used to merge \n",
    "            # if yes dont append, if not append \n",
    "            else: \n",
    "                print('longer than 100m')\n",
    "                old_set = np.array([prepare_set['start_vid'].iloc[0], prepare_set['end_vid'].iloc[0], prepare_set['length'].iloc[0], prepare_set['link_set'].iloc[0]])\n",
    "                new_sets.append(old_set)\n",
    "                link_used.append(link_set)\n",
    "                \n",
    "        # if it was not used and does not have next best, append it \n",
    "        else:\n",
    "            print('does not have next best')\n",
    "            old_set = np.array([prepare_set['start_vid'].iloc[0], prepare_set['end_vid'].iloc[0], prepare_set['length'].iloc[0], prepare_set['link_set'].iloc[0]])\n",
    "            new_sets.append(old_set)\n",
    "            link_used.append(link_set)\n",
    "    # if link_set as used, continue and dont append\n",
    "    else:\n",
    "        print('used')\n",
    "\n",
    "for i in range(len(new_sets)):\n",
    "    item = new_sets[i]\n",
    "    length = item[2].astype(float)\n",
    "    row = (item[0].astype(float), item[1].astype(float), length, item[3])\n",
    "    rows.append(row)\n",
    "        \n",
    "sql = '''insert into congestion.test_shortseg(start_vid, end_vid, length, link_set) VALUES %s'''    \n",
    "with con:\n",
    "    with con.cursor() as cur:\n",
    "        execute_values(cur, sql, rows)    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3603"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_sets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
